"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[3999],{8453(e,n,o){o.d(n,{R:()=>s,x:()=>a});var i=o(6540);const t={},r=i.createContext(t);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},8900(e,n,o){o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3/jetson-deployment","title":"Edge Deployment (Jetson Orin)","description":"Overview","source":"@site/docs/module-3/15-jetson-deployment.md","sourceDirName":"module-3","slug":"/module-3/jetson-deployment","permalink":"/ai-book/docs/module-3/jetson-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3/15-jetson-deployment.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"id":"jetson-deployment","title":"Edge Deployment (Jetson Orin)","sidebar_label":"Chapter 15 Jetson Deployment"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 14 Nav2 Humanoid","permalink":"/ai-book/docs/module-3/nav2-humanoid"},"next":{"title":"Chapter 16 Whisper Voice Control","permalink":"/ai-book/docs/module-4/whisper-voice-control"}}');var t=o(4848),r=o(8453);const s={id:"jetson-deployment",title:"Edge Deployment (Jetson Orin)",sidebar_label:"Chapter 15 Jetson Deployment"},a="Edge Deployment (Jetson Orin)",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Topics",id:"key-topics",level:2},{value:"Jetson Orin Hardware Setup",id:"jetson-orin-hardware-setup",level:2},{value:"Jetson Orin Platforms",id:"jetson-orin-platforms",level:3},{value:"Hardware Setup Steps",id:"hardware-setup-steps",level:3},{value:"TensorRT Integration for AI Models",id:"tensorrt-integration-for-ai-models",level:2},{value:"Python Example for TensorRT Optimization",id:"python-example-for-tensorrt-optimization",level:3},{value:"Performance Optimization Strategies",id:"performance-optimization-strategies",level:2},{value:"Memory Management for Jetson Platforms",id:"memory-management-for-jetson-platforms",level:3},{value:"TensorRT Optimization Process",id:"tensorrt-optimization-process",level:2},{value:"Deployment Strategies for Jetson Orin",id:"deployment-strategies-for-jetson-orin",level:2},{value:"Docker-based Deployment",id:"docker-based-deployment",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:2},{value:"Cross-Module Connection: Complete Pipeline Integration",id:"cross-module-connection-complete-pipeline-integration",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"edge-deployment-jetson-orin",children:"Edge Deployment (Jetson Orin)"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covers optimization and TensorRT integration for deploying robot perception systems on Jetson Orin platforms. The Jetson Orin series provides powerful AI acceleration for robotics applications while maintaining power efficiency."}),"\n",(0,t.jsx)(n.h2,{id:"key-topics",children:"Key Topics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Jetson Orin Nano and AGX optimization"}),"\n",(0,t.jsx)(n.li,{children:"TensorRT integration for AI models"}),"\n",(0,t.jsx)(n.li,{children:"Power and performance optimization"}),"\n",(0,t.jsx)(n.li,{children:"Deployment strategies"}),"\n",(0,t.jsx)(n.li,{children:"Performance monitoring"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"jetson-orin-hardware-setup",children:"Jetson Orin Hardware Setup"}),"\n",(0,t.jsx)(n.h3,{id:"jetson-orin-platforms",children:"Jetson Orin Platforms"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Platform"}),(0,t.jsx)(n.th,{children:"Compute Performance"}),(0,t.jsx)(n.th,{children:"Power Consumption"}),(0,t.jsx)(n.th,{children:"Memory"}),(0,t.jsx)(n.th,{children:"GPU"}),(0,t.jsx)(n.th,{children:"CPU"})]})}),(0,t.jsx)(n.tbody,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Jetson Orin Nano"}),(0,t.jsx)(n.td,{children:"Up to 40 TOPS"}),(0,t.jsx)(n.td,{children:"15W-25W"}),(0,t.jsx)(n.td,{children:"4GB-8GB LPDDR5"}),(0,t.jsx)(n.td,{children:"1024-core NVIDIA Ampere GPU"}),(0,t.jsx)(n.td,{children:"6-core ARM Cortex-A78AE v8.2 64-bit"})]})})]}),"\n",(0,t.jsx)(n.h3,{id:"hardware-setup-steps",children:"Hardware Setup Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Initial Setup"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install JetPack SDK (includes CUDA, cuDNN, TensorRT)\n# Download from NVIDIA Developer website\nsudo ./jetpack-installer.sh\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Verify GPU Access"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check GPU status\nsudo tegrastats\n\n# Verify CUDA installation\nnvcc --version\nnvidia-smi\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Configure Power Modes"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check available power modes\nsudo nvpmodel -q\n\n# Set power mode (e.g., MAXN for maximum performance)\nsudo nvpmodel -m 0\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tensorrt-integration-for-ai-models",children:"TensorRT Integration for AI Models"}),"\n",(0,t.jsx)(n.h3,{id:"python-example-for-tensorrt-optimization",children:"Python Example for TensorRT Optimization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example TensorRT optimization for robot perception\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\nimport onnx\nimport onnx_tensorrt.backend as backend\n\nclass TensorRTOptimizer:\n    def __init__(self):\n        self.logger = trt.Logger(trt.Logger.WARNING)\n        self.runtime = trt.Runtime(self.logger)\n\n    def optimize_model(self, onnx_model_path, engine_path, precision=\'fp16\'):\n        """\n        Optimize an ONNX model for TensorRT\n        """\n        # Create builder\n        builder = trt.Builder(self.logger)\n        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n        parser = trt.OnnxParser(network, self.logger)\n\n        # Parse ONNX model\n        with open(onnx_model_path, \'rb\') as model:\n            if not parser.parse(model.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return False\n\n        # Configure builder\n        config = builder.create_builder_config()\n\n        # Set precision\n        if precision == \'fp16\':\n            config.set_flag(trt.BuilderFlag.FP16)\n        elif precision == \'int8\':\n            config.set_flag(trt.BuilderFlag.INT8)\n            # Additional INT8 calibration would be needed here\n\n        # Optimize for Jetson\n        config.max_workspace_size = 2 << 30  # 2GB\n\n        # Build engine\n        serialized_engine = builder.build_serialized_network(network, config)\n\n        # Save optimized engine\n        with open(engine_path, \'wb\') as f:\n            f.write(serialized_engine)\n\n        return True\n\n    def load_engine(self, engine_path):\n        """\n        Load a TensorRT engine for inference\n        """\n        with open(engine_path, \'rb\') as f:\n            engine = self.runtime.deserialize_cuda_engine(f.read())\n\n        return engine\n\n# Example usage for robot perception\ndef optimize_perception_model():\n    optimizer = TensorRTOptimizer()\n\n    # Optimize a perception model (e.g., object detection)\n    success = optimizer.optimize_model(\n        onnx_model_path="robot_perception.onnx",\n        engine_path="robot_perception.trt",\n        precision=\'fp16\'  # Use FP16 for Jetson Orin\n    )\n\n    if success:\n        print("Model optimized successfully for Jetson deployment")\n    else:\n        print("Model optimization failed")\n\nif __name__ == "__main__":\n    optimize_perception_model()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-strategies",children:"Performance Optimization Strategies"}),"\n",(0,t.jsx)(n.h3,{id:"memory-management-for-jetson-platforms",children:"Memory Management for Jetson Platforms"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"// C++ example for efficient memory management on Jetson\n#include <cuda_runtime.h>\n#include <NvInfer.h>\n#include <iostream>\n#include <vector>\n\nclass JetsonMemoryManager {\npublic:\n    JetsonMemoryManager() {\n        // Initialize CUDA context\n        cudaSetDevice(0);\n\n        // Allocate pinned memory for faster CPU-GPU transfers\n        allocatePinnedMemory();\n\n        // Pre-allocate GPU memory pools\n        allocateMemoryPools();\n    }\n\n    ~JetsonMemoryManager() {\n        // Clean up allocated memory\n        cudaFreeHost(pinned_memory_);\n        cudaFree(gpu_memory_pool_);\n    }\n\nprivate:\n    void allocatePinnedMemory() {\n        // Allocate pinned memory for efficient transfers\n        size_t pinned_size = 1024 * 1024 * 10; // 10MB\n        cudaHostAlloc(&pinned_memory_, pinned_size, cudaHostAllocDefault);\n    }\n\n    void allocateMemoryPools() {\n        // Pre-allocate GPU memory to avoid runtime allocation\n        size_t gpu_pool_size = 1024 * 1024 * 100; // 100MB\n        cudaMalloc(&gpu_memory_pool_, gpu_pool_size);\n    }\n\n    void* pinned_memory_;\n    void* gpu_memory_pool_;\n};\n\n// TensorRT inference example optimized for Jetson\nclass JetsonTensorRTInference {\npublic:\n    JetsonTensorRTInference(const char* engine_path) {\n        // Load TensorRT engine\n        loadEngine(engine_path);\n\n        // Allocate I/O buffers\n        allocateBuffers();\n    }\n\n    void runInference(float* input_data, float* output_data) {\n        // Copy input to GPU\n        cudaMemcpy(input_buffer_, input_data, input_size_ * sizeof(float),\n                   cudaMemcpyHostToDevice);\n\n        // Run inference\n        context_->executeV2(bindings_.data());\n\n        // Copy output from GPU\n        cudaMemcpy(output_data, output_buffer_, output_size_ * sizeof(float),\n                   cudaMemcpyDeviceToHost);\n    }\n\nprivate:\n    void loadEngine(const char* engine_path) {\n        // Load serialized engine\n        // Implementation details for loading TensorRT engine\n    }\n\n    void allocateBuffers() {\n        // Allocate input and output buffers on GPU\n        cudaMalloc(&input_buffer_, input_size_ * sizeof(float));\n        cudaMalloc(&output_buffer_, output_size_ * sizeof(float));\n\n        // Set up bindings\n        bindings_.push_back(input_buffer_);\n        bindings_.push_back(output_buffer_);\n    }\n\n    nvinfer1::ICudaEngine* engine_;\n    nvinfer1::IExecutionContext* context_;\n    void* input_buffer_;\n    void* output_buffer_;\n    std::vector<void*> bindings_;\n    size_t input_size_;\n    size_t output_size_;\n};\n"})}),"\n",(0,t.jsx)(n.h2,{id:"tensorrt-optimization-process",children:"TensorRT Optimization Process"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Trained Model] --\x3e B[ONNX Conversion]\n    B --\x3e C[TensorRT Parser]\n    C --\x3e D[Optimization Passes]\n    D --\x3e E[TensorRT Engine]\n    E --\x3e F[Inference on Jetson]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"deployment-strategies-for-jetson-orin",children:"Deployment Strategies for Jetson Orin"}),"\n",(0,t.jsx)(n.h3,{id:"docker-based-deployment",children:"Docker-based Deployment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile for Jetson Orin deployment\nFROM nvcr.io/nvidia/ros:humble-ros-base-l4t-r35.2.1\n\n# Install additional dependencies\nRUN apt-get update && apt-get install -y \\\n    python3-pip \\\n    libopencv-dev \\\n    python3-opencv \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy model files\nCOPY models/ /app/models/\n\n# Copy application code\nCOPY src/ /app/src/\n\n# Install Python dependencies\nRUN pip3 install --no-cache-dir \\\n    torch \\\n    torchvision \\\n    tensorrt \\\n    pycuda\n\n# Set environment variables for Jetson\nENV CUDA_VISIBLE_DEVICES=0\nENV NVIDIA_VISIBLE_DEVICES=all\n\nWORKDIR /app\n\nCMD ["python3", "src/main.py"]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Monitor Jetson performance\nsudo tegrastats  # Real-time stats\n\n# Check power consumption\nsudo /usr/bin/jetson_clocks --show  # Show current clock speeds\n\n# Monitor thermal zones\ncat /sys/class/thermal/thermal_zone*/temp\n\n# Monitor GPU utilization\nsudo nvidia-smi -l 1\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optimize model architecture for target Jetson platform"}),"\n",(0,t.jsx)(n.li,{children:"Use TensorRT INT8 quantization for improved performance"}),"\n",(0,t.jsx)(n.li,{children:"Implement efficient memory management"}),"\n",(0,t.jsx)(n.li,{children:"Profile and monitor power consumption"}),"\n",(0,t.jsx)(n.li,{children:"Configure appropriate power modes based on application needs"}),"\n",(0,t.jsx)(n.li,{children:"Use CUDA streams for parallel processing"}),"\n",(0,t.jsx)(n.li,{children:"Pre-allocate memory pools to avoid runtime allocation overhead"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cross-module-connection-complete-pipeline-integration",children:"Cross-Module Connection: Complete Pipeline Integration"}),"\n",(0,t.jsx)(n.p,{children:"This deployment chapter ties together the entire AI-robot brain pipeline:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation to Reality"}),": Connects the Isaac Sim synthetic data generation (Chapter 12) with real-world deployment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Pipeline"}),": Deploys the Isaac ROS GEMs and VSLAM algorithms (Chapter 13) on edge hardware"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Integration"}),": Enables the Nav2 humanoid navigation (Chapter 14) to run efficiently on Jetson platforms"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The complete pipeline flows from digital twin simulation (Module 2) \u2192 synthetic data generation (Chapter 12) \u2192 perception algorithms (Chapter 13) \u2192 navigation planning (Chapter 14) \u2192 edge deployment (this chapter)."})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);