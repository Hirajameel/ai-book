"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[5228],{4059(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-4/capstone-system-design","title":"Capstone Part A - The Unified Architecture","description":"Overview","source":"@site/docs/module-4/19-capstone-system-design.md","sourceDirName":"module-4","slug":"/module-4/capstone-system-design","permalink":"/ai-book/docs/module-4/capstone-system-design","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/19-capstone-system-design.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"id":"capstone-system-design","title":"Capstone Part A - The Unified Architecture","sidebar_label":"Chapter 19 Capstone System Design"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 18 VLM Object Identification","permalink":"/ai-book/docs/module-4/vlm-object-identification"},"next":{"title":"Chapter 20 Capstone Final Execution","permalink":"/ai-book/docs/module-4/capstone-final-execution"}}');var i=t(4848),o=t(8453);const s={id:"capstone-system-design",title:"Capstone Part A - The Unified Architecture",sidebar_label:"Chapter 19 Capstone System Design"},r="Capstone Part A - The Unified Architecture",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Integration Map",id:"system-integration-map",level:2},{value:"Component Interaction Diagrams",id:"component-interaction-diagrams",level:2},{value:"Mermaid.js Flowchart: Complete VLA Pipeline",id:"mermaidjs-flowchart-complete-vla-pipeline",level:3},{value:"Mermaid.js: Real-time Processing Pipeline",id:"mermaidjs-real-time-processing-pipeline",level:3},{value:"Action Loop Implementation",id:"action-loop-implementation",level:2},{value:"Implementation Code",id:"implementation-code",level:3},{value:"Master Launch File",id:"master-launch-file",level:2},{value:"Hardware Integration Considerations",id:"hardware-integration-considerations",level:2},{value:"NVIDIA Jetson Orin Deployment",id:"nvidia-jetson-orin-deployment",level:3},{value:"RTX GPU Simulation",id:"rtx-gpu-simulation",level:3},{value:"Integration Validation",id:"integration-validation",level:2},{value:"Component Health Check",id:"component-health-check",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-part-a---the-unified-architecture",children:"Capstone Part A - The Unified Architecture"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This chapter focuses on connecting all components learned in previous modules: the Brain (Isaac), Body (URDF/Gazebo), and Voice (Whisper). We'll design the unified architecture that brings together the entire system into a cohesive autonomous humanoid."}),"\n",(0,i.jsx)(n.h2,{id:"system-integration-map",children:"System Integration Map"}),"\n",(0,i.jsx)(n.p,{children:"The Vision-Language-Action (VLA) system integrates four major components in a unified pipeline:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User Voice    \u2502    \u2502  LLM Reasoning   \u2502    \u2502  VLM Perception  \u2502\n\u2502   Command       \u2502\u2500\u2500\u2500\u25b6\u2502  (Task Planner)  \u2502\u2500\u2500\u2500\u25b6\u2502  (Object Finder) \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                  \u2502\n\u2502 "Go get the     \u2502    \u2502 - Task breakdown \u2502    \u2502 - Object         \u2502\n\u2502  red cup"       \u2502    \u2502 - Action seq.    \u2502    \u2502   detection      \u2502\n\u2502                 \u2502    \u2502 - Safety checks  \u2502    \u2502 - Spatial loc.   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Whisper Speech  \u2502    \u2502 ROS 2 Action     \u2502    \u2502 Isaac ROS        \u2502\n\u2502 Recognition     \u2502    \u2502 Sequencer        \u2502    \u2502 Navigation       \u2502\n\u2502                 \u2502    \u2502                  \u2502    \u2502                  \u2502\n\u2502 - Audio input   \u2502    \u2502 - Nav goals      \u2502    \u2502 - Path planning  \u2502\n\u2502 - Noise reduc.  \u2502    \u2502 - Manipulation   \u2502    \u2502 - Obstacle avoid.\u2502\n\u2502 - Text output   \u2502    \u2502 - Coordination   \u2502    \u2502 - Execution      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Robot Execution       \u2502\n                    \u2502   (Physical Actions)    \u2502\n                    \u2502                         \u2502\n                    \u2502 - Navigation            \u2502\n                    \u2502 - Manipulation          \u2502\n                    \u2502 - Feedback to user      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,i.jsx)(n.h2,{id:"component-interaction-diagrams",children:"Component Interaction Diagrams"}),"\n",(0,i.jsx)(n.h3,{id:"mermaidjs-flowchart-complete-vla-pipeline",children:"Mermaid.js Flowchart: Complete VLA Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[User Voice Command] --\x3e B(Whisper STT)\n    B --\x3e C{Command Type?}\n    C --\x3e|Navigation| D[LLM Task Planner]\n    C --\x3e|Object Find| E[VLM Object Detection]\n    C --\x3e|Complex Task| D\n\n    D --\x3e F[Generate Action Sequence]\n    E --\x3e G[Object Location Data]\n\n    F --\x3e H[ROS 2 Action Sequencer]\n    G --\x3e H\n\n    H --\x3e I[Isaac ROS Navigation]\n    I --\x3e J[Robot Execution]\n\n    J --\x3e K[Feedback to User]\n    K --\x3e L{Task Complete?}\n    L --\x3e|No| D\n    L --\x3e|Yes| M[End Task]\n\n    style A fill:#e1f5fe\n    style J fill:#f3e5f5\n    style K fill:#e8f5e8\n    style M fill:#fff3e0\n"})}),"\n",(0,i.jsx)(n.h3,{id:"mermaidjs-real-time-processing-pipeline",children:"Mermaid.js: Real-time Processing Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant User\n    participant Whisper as Whisper STT\n    participant LLM as LLM Planner\n    participant VLM as VLM Detector\n    participant ROS as ROS 2 Sequencer\n    participant Isaac as Isaac Navigation\n    participant Robot as Robot\n\n    User->>Whisper: Speak command\n    Whisper->>LLM: Transcribed text\n    Note over LLM: Reasoning phase\n    LLM->>VLM: Object query\n    VLM->>LLM: Object locations\n    LLM->>ROS: Action sequence\n    ROS->>Isaac: Navigation goals\n    Isaac->>Robot: Execute actions\n    Robot->>User: Task feedback\n"})}),"\n",(0,i.jsx)(n.h2,{id:"action-loop-implementation",children:"Action Loop Implementation"}),"\n",(0,i.jsx)(n.p,{children:'The core "Action Loop" follows this pattern:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio (Whisper) -> Text (LLM) -> Planner (Action Server) -> Execute (Nav2/Perception)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"implementation-code",children:"Implementation Code"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nimport json\nimport threading\nimport queue\n\nclass VLAMasterOrchestrator(Node):\n    def __init__(self):\n        super().__init__(\'vla_orchestrator\')\n\n        # Publishers and subscribers\n        self.voice_sub = self.create_subscription(String, \'voice_commands\', self.voice_callback, 10)\n        self.image_sub = self.create_subscription(Image, \'/camera/rgb/image_raw\', self.image_callback, 10)\n        self.llm_pub = self.create_publisher(String, \'llm_requests\', 10)\n        self.vlm_pub = self.create_publisher(String, \'vlm_requests\', 10)\n        self.action_pub = self.create_publisher(String, \'action_sequence\', 10)\n\n        # Internal queues for processing\n        self.voice_queue = queue.Queue()\n        self.image_queue = queue.Queue()\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self.process_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info("VLA Master Orchestrator initialized")\n\n    def voice_callback(self, msg):\n        """Handle incoming voice commands"""\n        self.get_logger().info(f"Received voice command: {msg.data}")\n        self.voice_queue.put(msg.data)\n\n    def image_callback(self, msg):\n        """Handle incoming images for VLM processing"""\n        # In a real implementation, we\'d process the image\n        # For now, we just store it for later use\n        self.image_queue.put(msg)\n\n    def process_loop(self):\n        """Main processing loop"""\n        while rclpy.ok():\n            try:\n                # Process voice commands\n                if not self.voice_queue.empty():\n                    voice_command = self.voice_queue.get()\n                    self.process_voice_command(voice_command)\n\n                # Process images for VLM\n                if not self.image_queue.empty():\n                    image_msg = self.image_queue.get()\n                    # Process image in background\n                    threading.Thread(target=self.process_image, args=(image_msg,)).start()\n\n                # Small delay to prevent busy waiting\n                import time\n                time.sleep(0.01)\n\n            except Exception as e:\n                self.get_logger().error(f"Error in processing loop: {e}")\n\n    def process_voice_command(self, command):\n        """Process voice command through LLM"""\n        # Create LLM request\n        llm_request = {\n            "command": command,\n            "context": "robotic_task_planning",\n            "required_outputs": ["action_sequence", "object_targets", "navigation_goals"]\n        }\n\n        # Send to LLM for processing\n        msg = String()\n        msg.data = json.dumps(llm_request)\n        self.llm_pub.publish(msg)\n        self.get_logger().info(f"Sent LLM request for command: {command}")\n\n    def process_image(self, image_msg):\n        """Process image for VLM object detection"""\n        # Create VLM request\n        vlm_request = {\n            "image_available": True,\n            "object_detection_needed": True,\n            "context": "environment_perception"\n        }\n\n        # Send to VLM for processing\n        msg = String()\n        msg.data = json.dumps(vlm_request)\n        self.vlm_pub.publish(msg)\n\n    def llm_response_callback(self, msg):\n        """Handle LLM responses with action sequences"""\n        try:\n            response_data = json.loads(msg.data)\n            action_sequence = response_data.get("action_sequence", [])\n\n            if action_sequence:\n                # Publish action sequence to execution system\n                action_msg = String()\n                action_msg.data = json.dumps(action_sequence)\n                self.action_pub.publish(action_msg)\n                self.get_logger().info(f"Published action sequence with {len(action_sequence)} actions")\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"Failed to parse LLM response: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    orchestrator = VLAMasterOrchestrator()\n\n    # Add LLM response subscription\n    orchestrator.llm_response_sub = orchestrator.create_subscription(\n        String,\n        \'llm_responses\',\n        orchestrator.llm_response_callback,\n        10\n    )\n\n    try:\n        rclpy.spin(orchestrator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        orchestrator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"master-launch-file",children:"Master Launch File"}),"\n",(0,i.jsx)(n.p,{children:'Here\'s the "Master Launch" file that initializes all nodes simultaneously:'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- vla_system.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Whisper Voice Node\n        Node(\n            package='whisper_voice_control',\n            executable='whisper_node',\n            name='whisper_voice_node',\n            output='screen',\n            parameters=[\n                {'model_size': 'base'},\n                {'sample_rate': 16000},\n            ]\n        ),\n\n        # LLM Task Planner Node\n        Node(\n            package='llm_task_planner',\n            executable='llm_task_planner_node',\n            name='llm_task_planner_node',\n            output='screen',\n            parameters=[\n                {'model_type': 'ollama'},\n                {'model_name': 'llama3:70b'},\n            ]\n        ),\n\n        # VLM Detection Node\n        Node(\n            package='vlm_detector',\n            executable='vlm_detection_node',\n            name='vlm_detection_node',\n            output='screen',\n            parameters=[\n                {'model_type': 'clip'},\n                {'confidence_threshold': 0.7},\n            ]\n        ),\n\n        # Isaac Navigation Node\n        Node(\n            package='isaac_ros_navigation',\n            executable='navigation_node',\n            name='navigation_node',\n            output='screen',\n            parameters=[\n                {'use_sim_time': False},\n            ]\n        ),\n\n        # VLA Master Orchestrator\n        Node(\n            package='vla_system',\n            executable='vla_orchestrator',\n            name='vla_orchestrator',\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"hardware-integration-considerations",children:"Hardware Integration Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"nvidia-jetson-orin-deployment",children:"NVIDIA Jetson Orin Deployment"}),"\n",(0,i.jsx)(n.p,{children:"For edge deployment on NVIDIA Jetson Orin:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Hardware-specific optimizations\nclass JetsonOptimizedVLA:\n    def __init__(self):\n        # Use TensorRT for optimized inference\n        self.use_tensorrt = True\n\n        # Optimize for limited memory\n        self.max_batch_size = 1  # Process one frame at a time\n\n        # Enable hardware acceleration\n        self.enable_gpu_processing = True\n        self.enable_dla = True  # Deep Learning Accelerator\n\n        # Power management\n        self.enable_power_optimization = True\n"})}),"\n",(0,i.jsx)(n.h3,{id:"rtx-gpu-simulation",children:"RTX GPU Simulation"}),"\n",(0,i.jsx)(n.p,{children:"For simulation with RTX GPU:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Simulation-specific optimizations\nclass RTXOptimizedVLA:\n    def __init__(self):\n        # Enable multi-GPU processing\n        self.enable_multi_gpu = True\n\n        # Higher batch sizes for better throughput\n        self.max_batch_size = 8\n\n        # Enable advanced rendering\n        self.enable_ray_tracing = True\n        self.enable_advanced_physics = True\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-validation",children:"Integration Validation"}),"\n",(0,i.jsx)(n.h3,{id:"component-health-check",children:"Component Health Check"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def validate_system_integration(self):\n    """Validate that all components are properly connected"""\n    checks = {\n        "whisper_node": self.check_node_health("whisper_voice_node"),\n        "llm_node": self.check_node_health("llm_task_planner_node"),\n        "vlm_node": self.check_node_health("vlm_detection_node"),\n        "navigation_node": self.check_node_health("navigation_node"),\n        "orchestrator": self.check_node_health("vla_orchestrator")\n    }\n\n    all_healthy = all(checks.values())\n\n    if all_healthy:\n        self.get_logger().info("All VLA components are healthy and connected")\n    else:\n        unhealthy = [name for name, healthy in checks.items() if not healthy]\n        self.get_logger().error(f"Unhealthy components: {unhealthy}")\n\n    return all_healthy\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.p,{children:"Monitor the complete VLA pipeline performance:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice-to-action latency"}),": Should be ",(0,i.jsx)(n.code,{children:"<3"})," seconds end-to-end"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object detection accuracy"}),": Should be >85% for known objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation success rate"}),": Should be >90% for clear paths"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System resource usage"}),": Should stay within hardware limits"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This unified architecture provides the foundation for a complete Vision-Language-Action system that can understand natural language commands, perceive its environment, and execute complex robotic tasks autonomously."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);