"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[8651],{8453(e,n,t){t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},9291(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/llm-task-planning","title":"LLMs as Robotic Task Planners","description":"Overview","source":"@site/docs/module-4/17-llm-task-planning.md","sourceDirName":"module-4","slug":"/module-4/llm-task-planning","permalink":"/ai-book/docs/module-4/llm-task-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/17-llm-task-planning.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"id":"llm-task-planning","title":"LLMs as Robotic Task Planners","sidebar_label":"Chapter 17 LLM Task Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 16 Whisper Voice Control","permalink":"/ai-book/docs/module-4/whisper-voice-control"},"next":{"title":"Chapter 18 VLM Object Identification","permalink":"/ai-book/docs/module-4/vlm-object-identification"}}');var o=t(4848),i=t(8453);const r={id:"llm-task-planning",title:"LLMs as Robotic Task Planners",sidebar_label:"Chapter 17 LLM Task Planning"},a="LLMs as Robotic Task Planners",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Development Environment Setup",id:"development-environment-setup",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:4},{value:"Software Requirements",id:"software-requirements",level:4},{value:"Installation Steps",id:"installation-steps",level:3},{value:"Alternative: OpenAI API Setup",id:"alternative-openai-api-setup",level:3},{value:"Reasoning-before-Acting Logic",id:"reasoning-before-acting-logic",level:2},{value:"Python Parser for LLM JSON Output",id:"python-parser-for-llm-json-output",level:2},{value:"System Prompt Template",id:"system-prompt-template",level:2},{value:"Integration with ROS 2 Action Servers",id:"integration-with-ros-2-action-servers",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"llms-as-robotic-task-planners",children:"LLMs as Robotic Task Planners"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This chapter focuses on using Large Language Models as cognitive bridges to translate natural language commands into ROS 2 action sequences. We'll explore prompt engineering techniques for generating ROS 2 Goal messages from natural language input."}),"\n",(0,o.jsx)(n.h2,{id:"development-environment-setup",children:"Development Environment Setup"}),"\n",(0,o.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before implementing the LLM integration, ensure your system meets the following requirements:"}),"\n",(0,o.jsx)(n.h4,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," (Edge deployment) or ",(0,o.jsx)(n.strong,{children:"RTX GPU"})," (Simulation)"]}),"\n",(0,o.jsx)(n.li,{children:"At least 8GB RAM (16GB recommended for local LLM inference)"}),"\n",(0,o.jsx)(n.li,{children:"50GB free disk space for models and dependencies"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"software-requirements",children:"Software Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS 2 Humble Hawksbill (or newer)"}),"\n",(0,o.jsx)(n.li,{children:"Python 3.8 or higher"}),"\n",(0,o.jsx)(n.li,{children:"CUDA 11.8+ (for GPU acceleration)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"installation-steps",children:"Installation Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install Ollama for local LLM inference"})," (recommended for privacy and reliability):"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"curl -fsSL https://ollama.ai/install.sh | sh\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pull required models"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# For reasoning tasks\nollama pull llama3:70b\n\n# Alternative smaller model for resource-constrained environments\nollama pull llama3:8b\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install Python dependencies"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai  # For both OpenAI API and Ollama compatibility\npip install pydantic\npip install ros2\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Verify LLM access"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Test Ollama\nollama run llama3:8b\n"})}),"\n",(0,o.jsx)(n.h3,{id:"alternative-openai-api-setup",children:"Alternative: OpenAI API Setup"}),"\n",(0,o.jsx)(n.p,{children:"If using OpenAI API instead of local inference:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Get API key"})," from ",(0,o.jsx)(n.a,{href:"https://platform.openai.com",children:"platform.openai.com"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Install dependencies"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Set environment variable"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"export OPENAI_API_KEY='your-api-key-here'\n"})}),"\n",(0,o.jsx)(n.h2,{id:"reasoning-before-acting-logic",children:"Reasoning-before-Acting Logic"}),"\n",(0,o.jsx)(n.p,{children:'The core concept of "reasoning-before-acting" involves having the LLM think through a command before generating action sequences:'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User Command: "Go to the kitchen and bring me a cup"\n                    \u2193\n        [LLM Reasoning Phase]\n    - Identify locations (kitchen)\n    - Identify objects (cup)\n    - Plan navigation sequence\n    - Plan manipulation sequence\n                    \u2193\n        [Action Sequence Generation]\n    1. Navigate to kitchen\n    2. Locate cup\n    3. Approach cup\n    4. Grasp cup\n    5. Return to user\n'})}),"\n",(0,o.jsx)(n.h2,{id:"python-parser-for-llm-json-output",children:"Python Parser for LLM JSON Output"}),"\n",(0,o.jsx)(n.p,{children:"Here's a Python parser that converts LLM JSON output into ROS 2 goals:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import json\nfrom typing import Dict, List, Any\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport rclpy\n\nclass LLMResponseParser:\n    def __init__(self, node: Node):\n        self.node = node\n        self.action_publisher = node.create_publisher(String, \'llm_actions\', 10)\n\n    def parse_llm_response(self, llm_output: str) -> Dict[str, Any]:\n        """\n        Parse LLM JSON response and convert to ROS 2 action sequence\n        Expected LLM output format:\n        {\n          "thoughts": "reasoning process...",\n          "actions": [\n            {\n              "type": "navigation",\n              "goal": {\n                "x": 1.0,\n                "y": 2.0,\n                "theta": 0.0\n              },\n              "description": "Go to kitchen"\n            },\n            {\n              "type": "object_detection",\n              "target": "cup",\n              "description": "Find the cup"\n            }\n          ]\n        }\n        """\n        try:\n            # Parse the JSON response from LLM\n            response_data = json.loads(llm_output)\n\n            # Validate the structure\n            if \'actions\' not in response_data:\n                raise ValueError("LLM response missing \'actions\' field")\n\n            # Convert to ROS 2 action sequence\n            action_sequence = self._convert_to_ros_actions(response_data[\'actions\'])\n\n            return action_sequence\n\n        except json.JSONDecodeError as e:\n            self.node.get_logger().error(f"Failed to parse LLM JSON response: {e}")\n            return {"error": f"JSON parsing error: {str(e)}"}\n        except Exception as e:\n            self.node.get_logger().error(f"Error processing LLM response: {e}")\n            return {"error": f"Processing error: {str(e)}"}\n\n    def _convert_to_ros_actions(self, actions: List[Dict]) -> Dict[str, Any]:\n        """Convert LLM action list to ROS 2 action sequence"""\n        ros_actions = []\n\n        for action in actions:\n            action_type = action.get(\'type\', \'unknown\')\n\n            if action_type == \'navigation\':\n                ros_action = self._create_navigation_action(action)\n            elif action_type == \'object_detection\':\n                ros_action = self._create_object_detection_action(action)\n            elif action_type == \'manipulation\':\n                ros_action = self._create_manipulation_action(action)\n            else:\n                ros_action = self._create_generic_action(action)\n\n            ros_actions.append(ros_action)\n\n        return {"actions": ros_actions, "count": len(ros_actions)}\n\n    def _create_navigation_action(self, action: Dict) -> Dict:\n        """Create a navigation action for ROS 2"""\n        goal = action.get(\'goal\', {})\n        x = goal.get(\'x\', 0.0)\n        y = goal.get(\'y\', 0.0)\n        theta = goal.get(\'theta\', 0.0)\n\n        # Create a PoseStamped goal for Nav2\n        nav_goal = {\n            "action_type": "navigation",\n            "goal_pose": {\n                "position": {"x": x, "y": y, "z": 0.0},\n                "orientation": self._euler_to_quaternion(0, 0, theta)\n            },\n            "description": action.get(\'description\', \'Navigate to goal\'),\n            "timeout": action.get(\'timeout\', 60.0)  # Default 60 seconds\n        }\n\n        return nav_goal\n\n    def _create_object_detection_action(self, action: Dict) -> Dict:\n        """Create an object detection action for ROS 2"""\n        return {\n            "action_type": "object_detection",\n            "target_object": action.get(\'target\', \'unknown\'),\n            "description": action.get(\'description\', \'Detect object\'),\n            "confidence_threshold": action.get(\'confidence_threshold\', 0.7)\n        }\n\n    def _create_manipulation_action(self, action: Dict) -> Dict:\n        """Create a manipulation action for ROS 2"""\n        return {\n            "action_type": "manipulation",\n            "action": action.get(\'manipulation_type\', \'grasp\'),\n            "target": action.get(\'target\', \'object\'),\n            "description": action.get(\'description\', \'Manipulate object\'),\n            "gripper_position": action.get(\'gripper_position\', 0.5)  # 0.0 to 1.0\n        }\n\n    def _create_generic_action(self, action: Dict) -> Dict:\n        """Create a generic action for ROS 2"""\n        return {\n            "action_type": "generic",\n            "command": action.get(\'command\', \'unknown\'),\n            "parameters": action.get(\'parameters\', {}),\n            "description": action.get(\'description\', \'Generic action\')\n        }\n\n    def _euler_to_quaternion(self, roll: float, pitch: float, yaw: float) -> Dict:\n        """Convert Euler angles to quaternion"""\n        import math\n\n        cy = math.cos(yaw * 0.5)\n        sy = math.sin(yaw * 0.5)\n        cp = math.cos(pitch * 0.5)\n        sp = math.sin(pitch * 0.5)\n        cr = math.cos(roll * 0.5)\n        sr = math.sin(roll * 0.5)\n\n        w = cr * cp * cy + sr * sp * sy\n        x = sr * cp * cy - cr * sp * sy\n        y = cr * sp * cy + sr * cp * sy\n        z = cr * cp * sy - sr * sp * cy\n\n        return {"x": x, "y": y, "z": z, "w": w}\n\n    def publish_action_sequence(self, action_sequence: Dict):\n        """Publish the action sequence to ROS 2 topic"""\n        msg = String()\n        msg.data = json.dumps(action_sequence)\n        self.action_publisher.publish(msg)\n        self.node.get_logger().info(f"Published action sequence with {action_sequence.get(\'count\', 0)} actions")\n\n# Example usage in a ROS 2 node\nclass LLMTaskPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_task_planner_node\')\n        self.parser = LLMResponseParser(self)\n\n        # Create subscriber for LLM responses\n        self.llm_subscriber = self.create_subscription(\n            String,\n            \'llm_responses\',\n            self.llm_response_callback,\n            10\n        )\n\n        self.get_logger().info("LLM Task Planner Node initialized")\n\n    def llm_response_callback(self, msg):\n        """Process incoming LLM responses"""\n        self.get_logger().info(f"Received LLM response: {msg.data[:100]}...")\n\n        # Parse the response\n        action_sequence = self.parser.parse_llm_response(msg.data)\n\n        if "error" not in action_sequence:\n            # Publish the action sequence\n            self.parser.publish_action_sequence(action_sequence)\n        else:\n            self.get_logger().error(f"Error processing LLM response: {action_sequence[\'error\']}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMTaskPlannerNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"system-prompt-template",children:"System Prompt Template"}),"\n",(0,o.jsx)(n.p,{children:'Here\'s a "System Prompt" template for the LLM to act as a reliable robot controller:'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'You are an expert robotic task planner. Your role is to convert natural language commands into structured action sequences for a humanoid robot.\n\nFollow these rules:\n1. Always think step-by-step before generating actions\n2. Consider safety and feasibility of each action\n3. Break complex tasks into simple, executable steps\n4. Use only the action types: navigation, object_detection, manipulation, generic\n5. Include necessary parameters for each action\n6. If information is missing, ask for clarification rather than guessing\n\nRespond in valid JSON format with this structure:\n{\n  "thoughts": "your reasoning process",\n  "actions": [\n    {\n      "type": "action_type",\n      "parameters": {...},\n      "description": "what this action does"\n    }\n  ]\n}\n\nExample: For "Go to the kitchen and bring me a red cup":\n{\n  "thoughts": "User wants a red cup from the kitchen. First navigate to kitchen, then find red cup, then grasp it, then return.",\n  "actions": [\n    {\n      "type": "navigation",\n      "goal": {"x": 5.0, "y": 3.0, "theta": 0.0},\n      "description": "Navigate to kitchen area"\n    },\n    {\n      "type": "object_detection",\n      "target": "red cup",\n      "description": "Locate the red cup in the environment"\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-ros-2-action-servers",children:"Integration with ROS 2 Action Servers"}),"\n",(0,o.jsx)(n.p,{children:"The generated action sequences need to be processed by ROS 2 action servers:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Action Server"}),": For ",(0,o.jsx)(n.code,{children:"MapsToPose"})," goals"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection Action Server"}),": For ",(0,o.jsx)(n.code,{children:"FindObject"})," goals"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation Action Server"}),": For ",(0,o.jsx)(n.code,{children:"PickObject"})," goals"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These will be implemented in the following chapters as part of the complete VLA system."}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"In the next sections, we'll implement the complete action execution pipeline that takes these parsed action sequences and executes them on the robot hardware."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);