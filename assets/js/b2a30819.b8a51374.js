"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[677],{7380(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/vlm-object-identification","title":"Vision-Language Models (VLM)","description":"Overview","source":"@site/docs/module-4/18-vlm-object-identification.md","sourceDirName":"module-4","slug":"/module-4/vlm-object-identification","permalink":"/ai-book/docs/module-4/vlm-object-identification","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/18-vlm-object-identification.md","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"id":"vlm-object-identification","title":"Vision-Language Models (VLM)","sidebar_label":"Chapter 18 VLM Object Identification"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 17 LLM Task Planning","permalink":"/ai-book/docs/module-4/llm-task-planning"},"next":{"title":"Chapter 19 Capstone System Design","permalink":"/ai-book/docs/module-4/capstone-system-design"}}');var s=i(4848),o=i(8453);const r={id:"vlm-object-identification",title:"Vision-Language Models (VLM)",sidebar_label:"Chapter 18 VLM Object Identification"},a="Vision-Language Models (VLM)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Development Environment Setup",id:"development-environment-setup",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:4},{value:"Software Requirements",id:"software-requirements",level:4},{value:"Installation Steps",id:"installation-steps",level:3},{value:"Zero-Shot Object Detection",id:"zero-shot-object-detection",level:2},{value:"CLIP-Based Object Detection",id:"clip-based-object-detection",level:3},{value:"Grounding DINO Integration",id:"grounding-dino-integration",level:2},{value:"Integration with Robotic System",id:"integration-with-robotic-system",level:2},{value:"ROS 2 Message Definitions",id:"ros-2-message-definitions",level:3},{value:"Spatial Reasoning Implementation",id:"spatial-reasoning-implementation",level:3},{value:"Confidence Thresholding",id:"confidence-thresholding",level:2},{value:"Handling Ambiguous Descriptions",id:"handling-ambiguous-descriptions",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Hardware Acceleration",id:"hardware-acceleration",level:3},{value:"Integration with VLA Pipeline",id:"integration-with-vla-pipeline",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"vision-language-models-vlm",children:"Vision-Language Models (VLM)"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:'This chapter covers Vision-Language Models for object identification based on verbal descriptions. We\'ll explore using models like CLIP and Grounding DINO to find objects by name ("Find the red cup") and integrate them with the robotic system.'}),"\n",(0,s.jsx)(n.h2,{id:"development-environment-setup",children:"Development Environment Setup"}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before implementing the VLM integration, ensure your system meets the following requirements:"}),"\n",(0,s.jsx)(n.h4,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," (Edge deployment) or ",(0,s.jsx)(n.strong,{children:"RTX GPU"})," (Simulation)"]}),"\n",(0,s.jsx)(n.li,{children:"At least 8GB RAM (16GB recommended for VLM inference)"}),"\n",(0,s.jsx)(n.li,{children:"CUDA-compatible GPU with 8GB+ VRAM for optimal performance"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"software-requirements",children:"Software Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble Hawksbill (or newer)"}),"\n",(0,s.jsx)(n.li,{children:"Python 3.8 or higher"}),"\n",(0,s.jsx)(n.li,{children:"CUDA 11.8+ (for GPU acceleration)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation-steps",children:"Installation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install PyTorch with CUDA support"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install CLIP dependencies"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai-clip\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install Grounding DINO dependencies"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install groundingdino-py\n# Alternative: Install from source for latest features\npip install git+https://github.com/IDEA-Research/GroundingDINO.git\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install additional computer vision dependencies"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install opencv-python\npip install supervision\npip install transformers\n"})}),"\n",(0,s.jsx)(n.h2,{id:"zero-shot-object-detection",children:"Zero-Shot Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"clip-based-object-detection",children:"CLIP-Based Object Detection"}),"\n",(0,s.jsx)(n.p,{children:"CLIP (Contrastive Language-Image Pre-training) enables zero-shot object detection by comparing image patches with text descriptions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import clip\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image as ImageMsg\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\n\nclass CLIPObjectDetector:\n    def __init__(self, node: Node):\n        self.node = node\n        self.bridge = CvBridge()\n\n        # Load pre-trained CLIP model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)\n\n        # Publishers and subscribers\n        self.detection_publisher = node.create_publisher(String, \'object_detections\', 10)\n\n    def detect_objects(self, image: np.ndarray, descriptions: list) -> dict:\n        """\n        Detect objects in image based on text descriptions using CLIP\n\n        Args:\n            image: Input image as numpy array\n            descriptions: List of object descriptions to search for\n\n        Returns:\n            Dictionary with detection results\n        """\n        # Convert image to PIL and preprocess\n        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n        # Tokenize text descriptions\n        text_inputs = torch.cat([clip.tokenize(f"a photo of a {desc}") for desc in descriptions]).to(self.device)\n\n        # Get model predictions\n        with torch.no_grad():\n            logits_per_image, logits_per_text = self.model(image_input, text_inputs)\n            probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n\n        # Create results dictionary\n        results = {\n            "objects": [],\n            "confidences": probs.tolist(),\n            "best_match": descriptions[np.argmax(probs)] if len(descriptions) > 0 else None,\n            "best_confidence": float(np.max(probs)) if len(probs) > 0 else 0.0\n        }\n\n        for i, desc in enumerate(descriptions):\n            results["objects"].append({\n                "description": desc,\n                "confidence": float(probs[i])\n            })\n\n        return results\n\nclass VLMDetectionNode(Node):\n    def __init__(self):\n        super().__init__(\'vlm_detection_node\')\n        self.clip_detector = CLIPObjectDetector(self)\n        self.bridge = CvBridge()\n\n        # Create subscribers and publishers\n        self.image_subscriber = self.create_subscription(\n            ImageMsg,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.command_subscriber = self.create_subscription(\n            String,\n            \'/object_search_commands\',\n            self.command_callback,\n            10\n        )\n\n        self.get_logger().info("VLM Detection Node initialized")\n\n    def command_callback(self, msg):\n        """Handle object search commands"""\n        try:\n            # Parse command - expecting JSON with "objects" list\n            import json\n            command_data = json.loads(msg.data)\n            objects_to_find = command_data.get("objects", [])\n\n            self.get_logger().info(f"Searching for objects: {objects_to_find}")\n\n            # For now, we\'ll use the last received image\n            # In a real implementation, you\'d want to process the current image\n            # or implement a more sophisticated image buffering system\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f"Invalid JSON command: {msg.data}")\n\n    def image_callback(self, msg):\n        """Process incoming camera images"""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Example: Detect common household objects\n            # In practice, this would come from the command callback\n            objects_to_find = ["cup", "bottle", "chair", "table", "person"]\n\n            # Perform object detection\n            results = self.clip_detector.detect_objects(cv_image, objects_to_find)\n\n            # Publish results\n            self.publish_detection_results(results)\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def publish_detection_results(self, results):\n        """Publish detection results to ROS topic"""\n        import json\n        msg = String()\n        msg.data = json.dumps(results)\n        self.clip_detector.detection_publisher.publish(msg)\n\n        # Log best match\n        if results.get("best_match"):\n            self.get_logger().info(\n                f"Best match: {results[\'best_match\']} (confidence: {results[\'best_confidence\']:.2f})"\n            )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLMDetectionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"grounding-dino-integration",children:"Grounding DINO Integration"}),"\n",(0,s.jsx)(n.p,{children:"Grounding DINO provides more precise object localization compared to CLIP:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport supervision as sv\nfrom groundingdino.util.inference import load_model, predict, annotate\nimport groundingdino.datasets.transforms as T\n\nclass GroundingDINOObjectDetector:\n    def __init__(self, node: Node, config_path: str, weights_path: str):\n        self.node = node\n        self.model = load_model(config_path, weights_path)\n        self.box_threshold = 0.35\n        self.text_threshold = 0.25\n\n    def detect_and_localize(self, image: np.ndarray, text_prompt: str) -> dict:\n        """\n        Detect and localize objects using Grounding DINO\n\n        Args:\n            image: Input image as numpy array (H, W, C)\n            text_prompt: Text description of objects to find\n\n        Returns:\n            Dictionary with bounding boxes and confidences\n        """\n        # Convert image to PIL\n        image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        # Transform image for model\n        transform = T.Compose([\n            T.RandomResize([800], max_size=1333),\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        image_transformed, _ = transform(image_pil, None)\n\n        # Run prediction\n        boxes, logits, phrases = predict(\n            model=self.model,\n            image=image_transformed,\n            caption=text_prompt,\n            box_threshold=self.box_threshold,\n            text_threshold=self.text_threshold\n        )\n\n        # Convert results to dictionary format\n        results = {\n            "boxes": boxes.tolist() if boxes.numel() > 0 else [],\n            "logits": logits.tolist() if logits.numel() > 0 else [],\n            "phrases": phrases,\n            "count": len(boxes) if boxes.numel() > 0 else 0\n        }\n\n        # Convert normalized coordinates to pixel coordinates\n        h, w, _ = image.shape\n        for i, box in enumerate(results["boxes"]):\n            # Box format: [cx, cy, w, h] in normalized coordinates\n            cx, cy, w_box, h_box = box\n            x1 = int((cx - w_box/2) * w)\n            y1 = int((cy - h_box/2) * h)\n            x2 = int((cx + w_box/2) * w)\n            y2 = int((cy + h_box/2) * h)\n            results["boxes"][i] = [x1, y1, x2, y2]\n\n        return results\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-robotic-system",children:"Integration with Robotic System"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-message-definitions",children:"ROS 2 Message Definitions"}),"\n",(0,s.jsx)(n.p,{children:"Create custom message types for object detection results:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# In your ROS 2 package, create msg/ObjectDetection.msg:\nstring object_name\nfloat32 confidence\nint32 x1\nint32 y1\nint32 x2\nint32 y2\nfloat32[] position_3d  # Optional: 3D position if available\n"})}),"\n",(0,s.jsx)(n.h3,{id:"spatial-reasoning-implementation",children:"Spatial Reasoning Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SpatialReasoning:\n    def __init__(self, node: Node):\n        self.node = node\n\n    def convert_2d_to_3d(self, bbox_2d: list, depth_image: np.ndarray, camera_info: dict) -> dict:\n        """\n        Convert 2D bounding box to 3D position using depth information\n\n        Args:\n            bbox_2d: [x1, y1, x2, y2] in pixel coordinates\n            depth_image: Depth image from RGB-D camera\n            camera_info: Camera intrinsic parameters\n\n        Returns:\n            3D position dictionary\n        """\n        x1, y1, x2, y2 = bbox_2d\n\n        # Calculate center of bounding box\n        center_x = int((x1 + x2) / 2)\n        center_y = int((y1 + y2) / 2)\n\n        # Get depth at center point (with some averaging for robustness)\n        depth_roi = depth_image[center_y-5:center_y+5, center_x-5:center_x+5]\n        avg_depth = np.nanmedian(depth_roi[depth_roi > 0])  # Ignore invalid depth values\n\n        # Convert pixel coordinates to 3D using camera intrinsics\n        if avg_depth > 0:\n            fx = camera_info[\'fx\']\n            fy = camera_info[\'fy\']\n            cx = camera_info[\'cx\']\n            cy = camera_info[\'cy\']\n\n            x_3d = (center_x - cx) * avg_depth / fx\n            y_3d = (center_y - cy) * avg_depth / fy\n            z_3d = avg_depth\n\n            return {\n                "x": float(x_3d),\n                "y": float(y_3d),\n                "z": float(z_3d),\n                "distance": float(avg_depth)\n            }\n\n        return {"x": 0.0, "y": 0.0, "z": 0.0, "distance": 0.0}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"confidence-thresholding",children:"Confidence Thresholding"}),"\n",(0,s.jsx)(n.p,{children:"Implement confidence-based filtering for reliable object identification:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def filter_detections_by_confidence(self, detections: dict, threshold: float = 0.7) -> dict:\n    """\n    Filter detections based on confidence threshold\n\n    Args:\n        detections: Raw detection results\n        threshold: Minimum confidence threshold (0.0 to 1.0)\n\n    Returns:\n        Filtered detection results\n    """\n    if "objects" in detections:\n        # For CLIP-based results\n        filtered_objects = []\n        for obj in detections["objects"]:\n            if obj["confidence"] >= threshold:\n                filtered_objects.append(obj)\n\n        detections["objects"] = filtered_objects\n        detections["filtered_count"] = len(filtered_objects)\n\n    elif "boxes" in detections and "logits" in detections:\n        # For Grounding DINO results\n        filtered_boxes = []\n        filtered_logits = []\n        filtered_phrases = []\n\n        for i, logit in enumerate(detections["logits"]):\n            if logit >= threshold:\n                filtered_boxes.append(detections["boxes"][i])\n                filtered_logits.append(logit)\n                filtered_phrases.append(detections["phrases"][i])\n\n        detections["boxes"] = filtered_boxes\n        detections["logits"] = filtered_logits\n        detections["phrases"] = filtered_phrases\n        detections["filtered_count"] = len(filtered_boxes)\n\n    return detections\n'})}),"\n",(0,s.jsx)(n.h2,{id:"handling-ambiguous-descriptions",children:"Handling Ambiguous Descriptions"}),"\n",(0,s.jsx)(n.p,{children:"Implement strategies for dealing with ambiguous object descriptions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ObjectDisambiguation:\n    def __init__(self, node: Node):\n        self.node = node\n\n    def handle_ambiguous_request(self, description: str, multiple_detections: list) -> dict:\n        """\n        Handle cases where multiple objects match the description\n\n        Args:\n            description: Original object description\n            multiple_detections: List of potential matches\n\n        Returns:\n            Resolution strategy or request for clarification\n        """\n        if len(multiple_detections) == 0:\n            return {\n                "action": "none_found",\n                "message": f"No objects matching \'{description}\' were found"\n            }\n        elif len(multiple_detections) == 1:\n            return {\n                "action": "accept",\n                "selected_object": multiple_detections[0]\n            }\n        else:\n            # Multiple matches found - need disambiguation\n            if self.is_spatially_descriptive(description):\n                # Use spatial reasoning to select the most appropriate object\n                return self.select_by_spatial_context(description, multiple_detections)\n            else:\n                # Request clarification from user\n                return {\n                    "action": "request_clarification",\n                    "message": f"Found multiple {description}s. Which one do you mean?",\n                    "options": self.describe_options(multiple_detections)\n                }\n\n    def is_spatially_descriptive(self, description: str) -> bool:\n        """Check if description contains spatial information"""\n        spatial_keywords = ["left", "right", "near", "far", "front", "back", "on", "under", "next to"]\n        desc_lower = description.lower()\n        return any(keyword in desc_lower for keyword in spatial_keywords)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Quantization"}),": Use INT8 quantization to reduce model size and improve inference speed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple frames together when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Cache results for similar queries to reduce redundant computation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-scale Processing"}),": Process at multiple scales to detect objects of different sizes"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"hardware-acceleration",children:"Hardware Acceleration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use TensorRT for NVIDIA GPUs to optimize inference performance"}),"\n",(0,s.jsx)(n.li,{children:"Consider OpenVINO for Intel hardware acceleration"}),"\n",(0,s.jsx)(n.li,{children:"For edge deployment on Jetson, use DeepStream SDK for optimized video processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vla-pipeline",children:"Integration with VLA Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"The VLM component integrates with the overall VLA system by:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Receiving camera feeds from the perception system"}),"\n",(0,s.jsx)(n.li,{children:"Processing verbal object descriptions from the LLM component"}),"\n",(0,s.jsx)(n.li,{children:"Publishing object locations to guide navigation and manipulation actions"}),"\n",(0,s.jsx)(n.li,{children:"Providing feedback to the orchestrator about detection confidence and success"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This enables the robot to understand and interact with its environment based on natural language descriptions, completing the Vision-Language-Action loop."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);