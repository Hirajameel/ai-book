"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[5904],{3627(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>_,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-1-robotic-nervous-system/python-ai-bridge","title":"Chapter 4 - Implementation of rclpy for AI-Agent Integration","description":"Bridging High-Level AI Logic with ROS Controllers","source":"@site/docs/module-1-robotic-nervous-system/python-ai-bridge.md","sourceDirName":"module-1-robotic-nervous-system","slug":"/module-1-robotic-nervous-system/python-ai-bridge","permalink":"/ai-book/docs/module-1-robotic-nervous-system/python-ai-bridge","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-robotic-nervous-system/python-ai-bridge.md","tags":[],"version":"current","frontMatter":{"title":"Chapter 4 - Implementation of rclpy for AI-Agent Integration","sidebar_label":"Chapter 4 Implementation of rclpy for AI-Agent Integration"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3 Deep Dive into Topics, Services, and Actions","permalink":"/ai-book/docs/module-1-robotic-nervous-system/communication-patterns"},"next":{"title":"Chapter 5 Robot Description Format and Skeletal Modeling","permalink":"/ai-book/docs/module-1-robotic-nervous-system/humanoid-kinematics-urdf"}}');var o=i(4848),s=i(8453);const r={title:"Chapter 4 - Implementation of rclpy for AI-Agent Integration",sidebar_label:"Chapter 4 Implementation of rclpy for AI-Agent Integration"},a="Chapter 4: Implementation of rclpy for AI-Agent Integration",l={},c=[{value:"Bridging High-Level AI Logic with ROS Controllers",id:"bridging-high-level-ai-logic-with-ros-controllers",level:2},{value:"Introduction to rclpy",id:"introduction-to-rclpy",level:2},{value:"Basic rclpy Node Structure",id:"basic-rclpy-node-structure",level:3},{value:"Integration with Popular AI Libraries",id:"integration-with-popular-ai-libraries",level:2},{value:"PyTorch Integration",id:"pytorch-integration",level:3},{value:"Humanoid Joint Controller with AI Integration",id:"humanoid-joint-controller-with-ai-integration",level:2},{value:"Advanced AI Integration Patterns",id:"advanced-ai-integration-patterns",level:2},{value:"Behavior Trees with ROS 2",id:"behavior-trees-with-ros-2",level:3},{value:"State Machines for Humanoid Behaviors",id:"state-machines-for-humanoid-behaviors",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Implementation of AI Agent Examples",id:"implementation-of-ai-agent-examples",level:2},{value:"1. Neural Network-based Decision Making AI",id:"1-neural-network-based-decision-making-ai",level:3},{value:"2. Rule-Based AI with Sensor Fusion",id:"2-rule-based-ai-with-sensor-fusion",level:3},{value:"3. Behavior Tree AI Implementation",id:"3-behavior-tree-ai-implementation",level:3},{value:"Integration Best Practices",id:"integration-best-practices",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-4-implementation-of-rclpy-for-ai-agent-integration",children:"Chapter 4: Implementation of rclpy for AI-Agent Integration"})}),"\n",(0,o.jsx)(n.h2,{id:"bridging-high-level-ai-logic-with-ros-controllers",children:"Bridging High-Level AI Logic with ROS Controllers"}),"\n",(0,o.jsx)(n.p,{children:"One of the key challenges in humanoid robotics is connecting sophisticated AI algorithms with low-level robot control systems. Python, with its rich ecosystem of AI libraries and the rclpy library for ROS 2, provides an excellent bridge between these two domains."}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-rclpy",children:"Introduction to rclpy"}),"\n",(0,o.jsx)(n.p,{children:"rclpy is the Python client library for ROS 2, providing a Python API for creating ROS 2 nodes, publishers, subscribers, services, and actions. It enables Python-based AI agents to seamlessly integrate with ROS 2-based robot control systems."}),"\n",(0,o.jsx)(n.h3,{id:"basic-rclpy-node-structure",children:"Basic rclpy Node Structure"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport torch  # Example AI library\nimport numpy as np\n\nclass AIBridgeNode(Node):\n    def __init__(self):\n        super().__init__(\'ai_bridge_node\')\n\n        # Create publisher for AI decisions\n        self.ai_publisher = self.create_publisher(String, \'ai_decisions\', 10)\n\n        # Create subscriber for sensor data\n        self.sensor_subscription = self.create_subscription(\n            String,\n            \'sensor_data\',\n            self.sensor_callback,\n            10\n        )\n\n        # Initialize AI model\n        self.ai_model = self.initialize_ai_model()\n\n        self.get_logger().info(\'AI Bridge Node initialized\')\n\n    def initialize_ai_model(self):\n        # Placeholder for AI model initialization\n        # In practice, this could be a neural network, rule-based system, etc.\n        self.get_logger().info(\'Initializing AI model...\')\n        return {"model_loaded": True, "type": "placeholder"}\n\n    def sensor_callback(self, msg):\n        # Process incoming sensor data\n        self.get_logger().info(f\'Received sensor data: {msg.data}\')\n\n        # Process with AI logic\n        ai_decision = self.process_with_ai(msg.data)\n\n        # Publish AI decision\n        decision_msg = String()\n        decision_msg.data = ai_decision\n        self.ai_publisher.publish(decision_msg)\n        self.get_logger().info(f\'Published AI decision: {ai_decision}\')\n\n    def process_with_ai(self, sensor_data):\n        # Placeholder for actual AI processing\n        # This is where the AI "thinking" happens\n        if "obstacle" in sensor_data.lower():\n            return "avoid_obstacle"\n        elif "person" in sensor_data.lower():\n            return "greet_person"\n        else:\n            return "continue_normal_operation"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_bridge_node = AIBridgeNode()\n\n    try:\n        rclpy.spin(ai_bridge_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_bridge_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-popular-ai-libraries",children:"Integration with Popular AI Libraries"}),"\n",(0,o.jsx)(n.h3,{id:"pytorch-integration",children:"PyTorch Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nimport torch\nimport torch.nn as nn\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionAINode(Node):\n    def __init__(self):\n        super().__init__('vision_ai_node')\n\n        # Initialize CV bridge for image processing\n        self.bridge = CvBridge()\n\n        # Create subscriber for camera images\n        self.image_subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for AI decisions\n        self.decision_publisher = self.create_publisher(String, 'ai_vision_decisions', 10)\n\n        # Load pre-trained model\n        self.model = self.load_vision_model()\n        self.model.eval()  # Set to evaluation mode\n\n        self.get_logger().info('Vision AI Node initialized')\n\n    def load_vision_model(self):\n        # Example: Load a pre-trained model\n        # In practice, you might load a custom trained model\n        try:\n            # Placeholder for actual model loading\n            # model = torch.load('path/to/your/model.pth')\n            # For this example, we'll create a simple model\n            class SimpleClassifier(nn.Module):\n                def __init__(self):\n                    super(SimpleClassifier, self).__init__()\n                    self.classifier = nn.Linear(3*224*224, 3)  # 3 classes example\n\n                def forward(self, x):\n                    x = x.view(x.size(0), -1)\n                    return self.classifier(x)\n\n            model = SimpleClassifier()\n            return model\n        except Exception as e:\n            self.get_logger().error(f'Failed to load model: {e}')\n            return None\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Preprocess image for the model\n            processed_image = self.preprocess_image(cv_image)\n\n            # Run inference\n            with torch.no_grad():\n                prediction = self.model(processed_image)\n                predicted_class = torch.argmax(prediction, dim=1)\n\n            # Convert prediction to action\n            action = self.prediction_to_action(predicted_class.item())\n\n            # Publish decision\n            decision_msg = String()\n            decision_msg.data = action\n            self.decision_publisher.publish(decision_msg)\n\n            self.get_logger().info(f'Vision AI decision: {action}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def preprocess_image(self, cv_image):\n        # Resize and normalize image\n        resized = cv2.resize(cv_image, (224, 224))\n        normalized = resized.astype(np.float32) / 255.0\n        tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)\n        return tensor\n\n    def prediction_to_action(self, class_id):\n        # Map prediction to robot action\n        actions = {\n            0: 'move_forward',\n            1: 'turn_left',\n            2: 'stop'\n        }\n        return actions.get(class_id, 'unknown')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_ai_node = VisionAINode()\n\n    try:\n        rclpy.spin(vision_ai_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_ai_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-joint-controller-with-ai-integration",children:"Humanoid Joint Controller with AI Integration"}),"\n",(0,o.jsx)(n.p,{children:'Here\'s a complete example of a "Humanoid Joint Controller" node that bridges AI decisions with joint control:'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float64MultiArray\nfrom sensor_msgs.msg import JointState\nfrom control_msgs.msg import JointTrajectoryControllerState\nimport numpy as np\nimport time\n\nclass HumanoidJointController(Node):\n    def __init__(self):\n        super().__init__('humanoid_joint_controller')\n\n        # Joint names for a simple humanoid (left leg)\n        self.joint_names = [\n            'left_hip_joint',\n            'left_knee_joint',\n            'left_ankle_joint',\n            'right_hip_joint',\n            'right_knee_joint',\n            'right_ankle_joint'\n        ]\n\n        # Current joint positions\n        self.current_positions = {name: 0.0 for name in self.joint_names}\n\n        # Create subscriber for AI decisions\n        self.ai_decision_sub = self.create_subscription(\n            String,\n            'ai_decisions',\n            self.ai_decision_callback,\n            10\n        )\n\n        # Create subscriber for current joint states\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        # Create publisher for joint commands\n        self.joint_cmd_pub = self.create_publisher(\n            JointTrajectoryControllerState,\n            'joint_commands',\n            10\n        )\n\n        # Timer for control loop\n        self.control_timer = self.create_timer(0.05, self.control_loop)  # 20 Hz\n\n        # AI state\n        self.current_ai_state = \"idle\"\n        self.target_positions = self.current_positions.copy()\n\n        self.get_logger().info('Humanoid Joint Controller initialized')\n\n    def ai_decision_callback(self, msg):\n        decision = msg.data\n        self.get_logger().info(f'AI decision received: {decision}')\n\n        # Update target positions based on AI decision\n        if decision == \"walk_forward\":\n            self.current_ai_state = \"walking\"\n            self.set_walking_pose()\n        elif decision == \"turn_left\":\n            self.current_ai_state = \"turning\"\n            self.set_turning_pose()\n        elif decision == \"balance\":\n            self.current_ai_state = \"balancing\"\n            self.set_balance_pose()\n        elif decision == \"idle\":\n            self.current_ai_state = \"idle\"\n            self.set_idle_pose()\n        else:\n            self.get_logger().warn(f'Unknown AI decision: {decision}')\n\n    def joint_state_callback(self, msg):\n        # Update current joint positions\n        for i, name in enumerate(msg.name):\n            if name in self.current_positions:\n                self.current_positions[name] = msg.position[i]\n\n    def set_walking_pose(self):\n        # Set target positions for walking motion\n        # This is a simplified example - real walking would be more complex\n        self.target_positions['left_hip_joint'] = 0.1\n        self.target_positions['left_knee_joint'] = -0.5\n        self.target_positions['left_ankle_joint'] = 0.4\n        self.target_positions['right_hip_joint'] = -0.1\n        self.target_positions['right_knee_joint'] = 0.5\n        self.target_positions['right_ankle_joint'] = -0.4\n\n    def set_turning_pose(self):\n        # Set target positions for turning motion\n        self.target_positions['left_hip_joint'] = 0.3\n        self.target_positions['left_knee_joint'] = -0.2\n        self.target_positions['left_ankle_joint'] = -0.1\n        self.target_positions['right_hip_joint'] = -0.3\n        self.target_positions['right_knee_joint'] = 0.2\n        self.target_positions['right_ankle_joint'] = 0.1\n\n    def set_balance_pose(self):\n        # Set target positions for balance maintenance\n        self.target_positions['left_hip_joint'] = 0.0\n        self.target_positions['left_knee_joint'] = 0.0\n        self.target_positions['left_ankle_joint'] = 0.0\n        self.target_positions['right_hip_joint'] = 0.0\n        self.target_positions['right_knee_joint'] = 0.0\n        self.target_positions['right_ankle_joint'] = 0.0\n\n    def set_idle_pose(self):\n        # Set target positions for idle state (standing)\n        self.target_positions['left_hip_joint'] = 0.0\n        self.target_positions['left_knee_joint'] = 0.0\n        self.target_positions['left_ankle_joint'] = 0.0\n        self.target_positions['right_hip_joint'] = 0.0\n        self.target_positions['right_knee_joint'] = 0.0\n        self.target_positions['right_ankle_joint'] = 0.0\n\n    def control_loop(self):\n        # Simple proportional controller to move joints toward targets\n        cmd_msg = JointTrajectoryControllerState()\n        cmd_msg.header.stamp = self.get_clock().now().to_msg()\n        cmd_msg.joint_names = self.joint_names\n\n        # Calculate command positions with simple control\n        cmd_positions = []\n        for name in self.joint_names:\n            current_pos = self.current_positions[name]\n            target_pos = self.target_positions[name]\n\n            # Simple proportional control\n            error = target_pos - current_pos\n            command_pos = current_pos + 0.1 * error  # 10% of error\n\n            cmd_positions.append(command_pos)\n\n        cmd_msg.desired.positions = cmd_positions\n        cmd_msg.actual.positions = list(self.current_positions.values())\n\n        # Publish command\n        self.joint_cmd_pub.publish(cmd_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = HumanoidJointController()\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-ai-integration-patterns",children:"Advanced AI Integration Patterns"}),"\n",(0,o.jsx)(n.h3,{id:"behavior-trees-with-ros-2",children:"Behavior Trees with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"Behavior trees provide a structured way to implement complex AI behaviors in robotics:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class BehaviorTreeNode:\n    def __init__(self, name):\n        self.name = name\n        self.status = "IDLE"  # IDLE, RUNNING, SUCCESS, FAILURE\n\n    def tick(self):\n        # Override in subclasses\n        pass\n\nclass SequenceNode(BehaviorTreeNode):\n    def __init__(self, name, children):\n        super().__init__(name)\n        self.children = children\n        self.current_child_idx = 0\n\n    def tick(self):\n        for i in range(self.current_child_idx, len(self.children)):\n            child = self.children[i]\n            child_status = child.tick()\n\n            if child_status == "RUNNING":\n                self.current_child_idx = i\n                return "RUNNING"\n            elif child_status == "FAILURE":\n                self.current_child_idx = 0\n                return "FAILURE"\n\n        self.current_child_idx = 0\n        return "SUCCESS"\n\nclass ActionNode(BehaviorTreeNode):\n    def __init__(self, name, ros_node, action_func):\n        super().__init__(name)\n        self.ros_node = ros_node\n        self.action_func = action_func\n\n    def tick(self):\n        return self.action_func()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"state-machines-for-humanoid-behaviors",children:"State Machines for Humanoid Behaviors"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from enum import Enum\n\nclass HumanoidState(Enum):\n    IDLE = "idle"\n    WALKING = "walking"\n    BALANCING = "balancing"\n    INTERACTING = "interacting"\n    EMERGENCY = "emergency"\n\nclass HumanoidStateMachine:\n    def __init__(self, ros_node):\n        self.ros_node = ros_node\n        self.current_state = HumanoidState.IDLE\n        self.state_start_time = time.time()\n\n    def transition_to(self, new_state):\n        if self.current_state != new_state:\n            self.ros_node.get_logger().info(f\'State transition: {self.current_state.value} -> {new_state.value}\')\n            self.current_state = new_state\n            self.state_start_time = time.time()\n\n    def update(self, sensor_data):\n        # State transition logic based on sensor data and AI decisions\n        if "emergency" in sensor_data or "falling" in sensor_data:\n            self.transition_to(HumanoidState.EMERGENCY)\n        elif "balance" in sensor_data:\n            self.transition_to(HumanoidState.BALANCING)\n        elif "walk" in sensor_data:\n            self.transition_to(HumanoidState.WALKING)\n        elif "interact" in sensor_data:\n            self.transition_to(HumanoidState.INTERACTING)\n        else:\n            self.transition_to(HumanoidState.IDLE)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,o.jsxs)(n.admonition,{type:"info",children:[(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Hardware Note"}),": When running AI models on Jetson Orin Nano:"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use TensorRT for optimized inference when possible"}),"\n",(0,o.jsx)(n.li,{children:"Consider model quantization to reduce computational requirements"}),"\n",(0,o.jsx)(n.li,{children:"Monitor GPU memory usage to prevent out-of-memory errors"}),"\n",(0,o.jsx)(n.li,{children:"Implement proper error handling for failed AI inferences"}),"\n"]})]}),"\n",(0,o.jsx)(n.h2,{id:"implementation-of-ai-agent-examples",children:"Implementation of AI Agent Examples"}),"\n",(0,o.jsx)(n.p,{children:"Here are complete, validated examples of AI agents that interface with ROS 2 controllers:"}),"\n",(0,o.jsx)(n.h3,{id:"1-neural-network-based-decision-making-ai",children:"1. Neural Network-based Decision Making AI"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32MultiArray\nfrom sensor_msgs.msg import JointState, Image\nfrom cv_bridge import CvBridge\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass NeuralNetworkAINode(Node):\n    def __init__(self):\n        super().__init__(\'neural_network_ai\')\n\n        # Initialize CV bridge for image processing\n        self.bridge = CvBridge()\n\n        # Create publishers\n        self.decision_publisher = self.create_publisher(String, \'ai_decisions\', 10)\n        self.motor_command_publisher = self.create_publisher(Float32MultiArray, \'motor_commands\', 10)\n\n        # Create subscribers\n        self.joint_state_subscriber = self.create_subscription(\n            JointState, \'joint_states\', self.joint_state_callback, 10)\n        self.image_subscriber = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.imu_subscriber = self.create_subscription(\n            String, \'imu_data\', self.imu_callback, 10)\n\n        # Initialize neural network\n        self.ai_model = self.create_simple_nn()\n\n        # Robot state\n        self.current_joint_positions = {}\n        self.current_image = None\n        self.current_imu_data = None\n        self.ai_state = "idle"\n\n        # Timer for AI processing\n        self.ai_timer = self.create_timer(0.1, self.process_with_ai)\n\n        self.get_logger().info(\'Neural Network AI Node initialized\')\n\n    def create_simple_nn(self):\n        """Create a simple neural network for demonstration"""\n        class SimpleNN(nn.Module):\n            def __init__(self, input_size, hidden_size, output_size):\n                super(SimpleNN, self).__init__()\n                self.fc1 = nn.Linear(input_size, hidden_size)\n                self.relu = nn.ReLU()\n                self.fc2 = nn.Linear(hidden_size, output_size)\n\n            def forward(self, x):\n                x = self.fc1(x)\n                x = self.relu(x)\n                x = self.fc2(x)\n                return x\n\n        # Input: 10 joint positions + 6 IMU values = 16\n        # Output: 6 motor commands (for 6 joints)\n        model = SimpleNN(input_size=16, hidden_size=32, output_size=6)\n        return model\n\n    def joint_state_callback(self, msg):\n        """Update current joint positions"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.current_joint_positions[name] = msg.position[i]\n\n    def image_callback(self, msg):\n        """Process camera image"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.current_image = cv_image\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        try:\n            # In a real system, this would parse actual IMU message\n            # For this example, we\'ll just store the string\n            self.current_imu_data = msg.data\n        except Exception as e:\n            self.get_logger().error(f\'Error processing IMU: {e}\')\n\n    def process_with_ai(self):\n        """Process sensor data through neural network"""\n        # Prepare input vector\n        input_vector = self.prepare_input_vector()\n\n        if input_vector is not None and len(input_vector) == 16:\n            # Convert to tensor\n            input_tensor = torch.FloatTensor(input_vector).unsqueeze(0)\n\n            # Run through neural network\n            with torch.no_grad():\n                output = self.ai_model(input_tensor)\n                motor_commands = output.squeeze(0).numpy()\n\n            # Publish motor commands\n            cmd_msg = Float32MultiArray()\n            cmd_msg.data = motor_commands.tolist()\n            self.motor_command_publisher.publish(cmd_msg)\n\n            # Make high-level decision based on commands\n            decision = self.interpret_commands(motor_commands)\n            decision_msg = String()\n            decision_msg.data = decision\n            self.decision_publisher.publish(decision_msg)\n\n            self.get_logger().info(f\'AI decision: {decision}, Commands: {motor_commands[:3]}...\')\n\n    def prepare_input_vector(self):\n        """Prepare input vector from sensor data"""\n        # Get joint positions (6 main joints for example)\n        joint_names = [\'left_hip_joint\', \'left_knee_joint\', \'left_ankle_joint\',\n                      \'right_hip_joint\', \'right_knee_joint\', \'right_ankle_joint\']\n\n        joint_positions = []\n        for name in joint_names:\n            pos = self.current_joint_positions.get(name, 0.0)\n            joint_positions.append(pos)\n\n        # Get IMU data (simplified - would parse real IMU message in practice)\n        imu_values = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # [orientation, angular_vel, linear_acc]\n\n        # Combine into input vector\n        input_vector = joint_positions + imu_values\n        return input_vector if len(input_vector) == 16 else None\n\n    def interpret_commands(self, motor_commands):\n        """Interpret motor commands for high-level decision"""\n        # Simple interpretation based on command magnitudes\n        avg_command = np.mean(np.abs(motor_commands))\n\n        if avg_command < 0.1:\n            return "idle"\n        elif avg_command < 0.5:\n            return "slow_movement"\n        else:\n            return "active_movement"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_node = NeuralNetworkAINode()\n\n    try:\n        rclpy.spin(ai_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-rule-based-ai-with-sensor-fusion",children:"2. Rule-Based AI with Sensor Fusion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32\nfrom sensor_msgs.msg import JointState, Imu\nimport math\n\nclass RuleBasedAINode(Node):\n    def __init__(self):\n        super().__init__(\'rule_based_ai\')\n\n        # Publishers\n        self.decision_publisher = self.create_publisher(String, \'ai_decisions\', 10)\n        self.warning_publisher = self.create_publisher(String, \'ai_warnings\', 10)\n\n        # Subscribers\n        self.joint_subscriber = self.create_subscription(\n            JointState, \'joint_states\', self.joint_callback, 10)\n        self.imu_subscriber = self.create_subscription(\n            Imu, \'imu_data\', self.imu_callback, 10)\n        self.distance_subscriber = self.create_subscription(\n            Float32, \'distance_to_obstacle\', self.distance_callback, 10)\n\n        # Robot state\n        self.joint_positions = {}\n        self.balance_state = {\'roll\': 0.0, \'pitch\': 0.0, \'yaw\': 0.0}\n        self.obstacle_distance = float(\'inf\')\n        self.current_behavior = "idle"\n        self.balance_threshold = 0.2  # radians\n        self.obstacle_threshold = 1.0  # meters\n\n        # Timer for rule evaluation\n        self.rule_timer = self.create_timer(0.05, self.evaluate_rules)\n\n        self.get_logger().info(\'Rule-Based AI Node initialized\')\n\n    def joint_callback(self, msg):\n        """Update joint positions"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def imu_callback(self, msg):\n        """Process IMU data for balance state"""\n        # Convert quaternion to roll/pitch/yaw (simplified)\n        # In practice, use proper quaternion to Euler conversion\n        orientation = msg.orientation\n        self.balance_state[\'roll\'] = math.atan2(2.0 * (orientation.w * orientation.x + orientation.y * orientation.z),\n                                               1.0 - 2.0 * (orientation.x * orientation.x + orientation.y * orientation.y))\n        self.balance_state[\'pitch\'] = math.asin(2.0 * (orientation.w * orientation.y - orientation.z * orientation.x))\n        self.balance_state[\'yaw\'] = math.atan2(2.0 * (orientation.w * orientation.z + orientation.x * orientation.y),\n                                              1.0 - 2.0 * (orientation.y * orientation.y + orientation.z * orientation.z))\n\n    def distance_callback(self, msg):\n        """Update obstacle distance"""\n        self.obstacle_distance = msg.data\n\n    def evaluate_rules(self):\n        """Evaluate rules and make decisions"""\n        # Rule 1: Check balance\n        is_unbalanced = (abs(self.balance_state[\'roll\']) > self.balance_threshold or\n                        abs(self.balance_state[\'pitch\']) > self.balance_threshold)\n\n        # Rule 2: Check for obstacles\n        is_obstacle_close = self.obstacle_distance < self.obstacle_threshold\n\n        # Rule 3: Check joint positions for potential issues\n        extreme_joint_positions = any(abs(pos) > 1.5 for pos in self.joint_positions.values())\n\n        # Decision making based on rules\n        decision = "idle"\n        warning = ""\n\n        if is_unbalanced:\n            decision = "balance_correction"\n            warning = "Robot is unbalanced - adjusting posture"\n        elif is_obstacle_close:\n            decision = "avoid_obstacle"\n            warning = f"Obstacle detected at {self.obstacle_distance:.2f}m - avoiding"\n        elif extreme_joint_positions:\n            decision = "safe_mode"\n            warning = "Extreme joint positions detected - moving to safe position"\n        else:\n            # Default behavior based on current state\n            if self.current_behavior == "walking":\n                decision = "continue_walking"\n            elif self.current_behavior == "turning":\n                decision = "continue_turning"\n            else:\n                decision = "idle"\n\n        # Publish decision\n        decision_msg = String()\n        decision_msg.data = decision\n        self.decision_publisher.publish(decision_msg)\n\n        # Publish warning if needed\n        if warning:\n            warning_msg = String()\n            warning_msg.data = warning\n            self.warning_publisher.publish(warning_msg)\n\n        self.get_logger().info(f\'Rule-based decision: {decision}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_node = RuleBasedAINode()\n\n    try:\n        rclpy.spin(ai_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-behavior-tree-ai-implementation",children:"3. Behavior Tree AI Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState\nimport time\nfrom enum import Enum\n\nclass BehaviorStatus(Enum):\n    SUCCESS = "success"\n    FAILURE = "failure"\n    RUNNING = "running"\n\nclass BehaviorNode:\n    """Base class for behavior tree nodes"""\n    def __init__(self, name):\n        self.name = name\n        self.status = BehaviorStatus.RUNNING\n\n    def tick(self, blackboard):\n        """Execute the behavior and return status"""\n        raise NotImplementedError\n\nclass SequenceNode(BehaviorNode):\n    """Sequence node - executes children in order until one fails"""\n    def __init__(self, name, children):\n        super().__init__(name)\n        self.children = children\n        self.current_child_idx = 0\n\n    def tick(self, blackboard):\n        for i in range(self.current_child_idx, len(self.children)):\n            child = self.children[i]\n            child_status = child.tick(blackboard)\n\n            if child_status == BehaviorStatus.FAILURE:\n                self.current_child_idx = 0\n                return BehaviorStatus.FAILURE\n            elif child_status == BehaviorStatus.RUNNING:\n                self.current_child_idx = i\n                return BehaviorStatus.RUNNING\n\n        self.current_child_idx = 0\n        return BehaviorStatus.SUCCESS\n\nclass SelectorNode(BehaviorNode):\n    """Selector node - executes children in order until one succeeds"""\n    def __init__(self, name, children):\n        super().__init__(name)\n        self.children = children\n        self.current_child_idx = 0\n\n    def tick(self, blackboard):\n        for i in range(self.current_child_idx, len(self.children)):\n            child = self.children[i]\n            child_status = child.tick(blackboard)\n\n            if child_status == BehaviorStatus.SUCCESS:\n                self.current_child_idx = 0\n                return BehaviorStatus.SUCCESS\n            elif child_status == BehaviorStatus.RUNNING:\n                self.current_child_idx = i\n                return BehaviorStatus.RUNNING\n\n        self.current_child_idx = 0\n        return BehaviorStatus.FAILURE\n\nclass CheckBalanceNode(BehaviorNode):\n    """Check if robot is balanced"""\n    def __init__(self, name):\n        super().__init__(name)\n\n    def tick(self, blackboard):\n        # Check balance from blackboard (simplified)\n        roll = blackboard.get(\'roll\', 0.0)\n        pitch = blackboard.get(\'pitch\', 0.0)\n        threshold = 0.2\n\n        if abs(roll) < threshold and abs(pitch) < threshold:\n            return BehaviorStatus.SUCCESS\n        else:\n            return BehaviorStatus.FAILURE\n\nclass BalanceRobotNode(BehaviorNode):\n    """Attempt to balance the robot"""\n    def __init__(self, name):\n        super().__init__(name)\n\n    def tick(self, blackboard):\n        # In real implementation, this would send balance commands\n        # For simulation, we\'ll just return RUNNING\n        return BehaviorStatus.RUNNING\n\nclass WalkForwardNode(BehaviorNode):\n    """Walk forward behavior"""\n    def __init__(self, name):\n        super().__init__(name)\n\n    def tick(self, blackboard):\n        # In real implementation, this would send walking commands\n        # For simulation, we\'ll return SUCCESS after a few ticks\n        walk_counter = blackboard.get(\'walk_counter\', 0)\n        blackboard[\'walk_counter\'] = walk_counter + 1\n\n        if walk_counter < 10:  # Walk for 10 ticks\n            return BehaviorStatus.RUNNING\n        else:\n            blackboard[\'walk_counter\'] = 0\n            return BehaviorStatus.SUCCESS\n\nclass BehaviorTreeAINode(Node):\n    """AI node implementing a behavior tree for humanoid robot control"""\n    def __init__(self):\n        super().__init__(\'behavior_tree_ai\')\n\n        # Publishers and subscribers\n        self.decision_publisher = self.create_publisher(String, \'ai_decisions\', 10)\n        self.joint_subscriber = self.create_subscription(\n            JointState, \'joint_states\', self.joint_callback, 10)\n\n        # Blackboard for sharing state between behavior nodes\n        self.blackboard = {\n            \'roll\': 0.0,\n            \'pitch\': 0.0,\n            \'walk_counter\': 0\n        }\n\n        # Create behavior tree\n        self.create_behavior_tree()\n\n        # Timer for tree execution\n        self.tree_timer = self.create_timer(0.1, self.execute_behavior_tree)\n\n        self.get_logger().info(\'Behavior Tree AI Node initialized\')\n\n    def create_behavior_tree(self):\n        """Create the behavior tree structure"""\n        # Balance check and correction sequence\n        balance_sequence = SequenceNode("balance_sequence", [\n            CheckBalanceNode("check_balance"),\n            BalanceRobotNode("balance_robot")\n        ])\n\n        # Main behavior selector (balance check has priority)\n        self.behavior_tree = SelectorNode("main_selector", [\n            balance_sequence,  # This has priority - will try to balance first\n            WalkForwardNode("walk_forward")  # If balanced, walk forward\n        ])\n\n    def joint_callback(self, msg):\n        """Update blackboard with joint state data"""\n        # In a real implementation, this would extract balance-related\n        # information from joint states and IMU data\n        pass\n\n    def execute_behavior_tree(self):\n        """Execute the behavior tree"""\n        status = self.behavior_tree.tick(self.blackboard)\n\n        # Publish decision based on tree status\n        decision_msg = String()\n        decision_msg.data = f"behavior_tree_status_{status.value}"\n        self.decision_publisher.publish(decision_msg)\n\n        self.get_logger().info(f\'Behavior tree status: {status.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    ai_node = BehaviorTreeAINode()\n\n    try:\n        rclpy.spin(ai_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        ai_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-best-practices",children:"Integration Best Practices"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Handling"}),": Always implement robust error handling for AI model failures"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fallback Behaviors"}),": Design safe fallback behaviors when AI is unavailable"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Management"}),": Consider AI processing latency in real-time control loops"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Monitor CPU/GPU usage and implement throttling if needed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Ensure AI decisions don't compromise robot or human safety"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter demonstrated how to bridge high-level AI logic with ROS 2 controllers using rclpy. We covered integration with popular AI libraries like PyTorch, implemented a complete humanoid joint controller, and discussed advanced patterns like behavior trees and state machines. The next chapter will focus on URDF modeling for humanoid robots, which provides the physical representation needed for these AI-controlled systems to operate effectively."})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);