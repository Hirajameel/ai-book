"use strict";(globalThis.webpackChunkfrontend_docu=globalThis.webpackChunkfrontend_docu||[]).push([[6420],{7996(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/whisper-voice-control","title":"Voice Intelligence with Whisper","description":"Overview","source":"@site/docs/module-4/16-whisper-voice-control.md","sourceDirName":"module-4","slug":"/module-4/whisper-voice-control","permalink":"/ai-book/docs/module-4/whisper-voice-control","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/16-whisper-voice-control.md","tags":[],"version":"current","sidebarPosition":16,"frontMatter":{"id":"whisper-voice-control","title":"Voice Intelligence with Whisper","sidebar_label":"Chapter 16 Whisper Voice Control"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 15 Jetson Deployment","permalink":"/ai-book/docs/module-3/jetson-deployment"},"next":{"title":"Chapter 17 LLM Task Planning","permalink":"/ai-book/docs/module-4/llm-task-planning"}}');var r=i(4848),o=i(8453);const a={id:"whisper-voice-control",title:"Voice Intelligence with Whisper",sidebar_label:"Chapter 16 Whisper Voice Control"},t="Voice Intelligence with Whisper",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Installing OpenAI Whisper",id:"installing-openai-whisper",level:2},{value:"Basic Whisper Integration with ROS 2",id:"basic-whisper-integration-with-ros-2",level:2},{value:"Creating the ROS 2 Package",id:"creating-the-ros-2-package",level:2},{value:"Bridge Architecture",id:"bridge-architecture",level:2},{value:"Testing the Voice Command Bridge",id:"testing-the-voice-command-bridge",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Performance Considerations",id:"performance-considerations-1",level:2},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"voice-intelligence-with-whisper",children:"Voice Intelligence with Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This chapter covers the integration of OpenAI Whisper for real-time voice-to-text capabilities in the humanoid robot system. We'll explore both noise reduction techniques and compare local vs. cloud-based Speech-to-Text approaches."}),"\n",(0,r.jsx)(n.h2,{id:"installing-openai-whisper",children:"Installing OpenAI Whisper"}),"\n",(0,r.jsx)(n.p,{children:"First, let's install the OpenAI Whisper library and its dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,r.jsx)(n.p,{children:"You may also need to install additional dependencies for audio processing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install torch torchaudio\n"})}),"\n",(0,r.jsx)(n.p,{children:"For audio recording capabilities, install PyAudio:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install pyaudio\n"})}),"\n",(0,r.jsx)(n.h2,{id:"basic-whisper-integration-with-ros-2",children:"Basic Whisper Integration with ROS 2"}),"\n",(0,r.jsx)(n.p,{children:"Now let's create a ROS 2 node that captures audio from a microphone and processes it with Whisper:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport whisper\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nfrom std_msgs.msg import String\n\nclass WhisperVoiceNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_voice_node\')\n\n        # Create publisher for voice commands\n        self.voice_publisher = self.create_publisher(String, \'voice_commands\', 10)\n\n        # Load Whisper model\n        self.model = whisper.load_model("base")  # You can use "tiny", "base", "small", "medium", "large"\n\n        # Audio parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000  # Whisper works best at 16kHz\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # Create audio stream\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        # Start audio processing thread\n        self.audio_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info("Whisper Voice Node initialized")\n\n    def process_audio(self):\n        while rclpy.ok():\n            # Read audio data\n            data = self.stream.read(self.chunk)\n            audio_array = np.frombuffer(data, dtype=np.int16)\n\n            # Normalize audio\n            audio_array = audio_array.astype(np.float32) / 32768.0\n\n            # Add to queue for processing\n            self.audio_queue.put(audio_array)\n\n            # Process accumulated audio if queue has enough data\n            if self.audio_queue.qsize() > 10:  # Process every 10 chunks (about 0.6 seconds)\n                self.transcribe_audio()\n\n    def transcribe_audio(self):\n        # Collect audio data from queue\n        audio_data = []\n        while not self.audio_queue.empty():\n            audio_data.append(self.audio_queue.get())\n\n        if len(audio_data) > 0:\n            # Concatenate audio data\n            full_audio = np.concatenate(audio_data)\n\n            # Transcribe with Whisper\n            result = self.model.transcribe(full_audio)\n            text = result["text"].strip()\n\n            if text:  # Only publish if there\'s actual text\n                msg = String()\n                msg.data = text\n                self.voice_publisher.publish(msg)\n                self.get_logger().info(f"Published voice command: {text}")\n\n    def destroy_node(self):\n        # Clean up audio resources\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperVoiceNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"creating-the-ros-2-package",children:"Creating the ROS 2 Package"}),"\n",(0,r.jsx)(n.p,{children:"Create a new ROS 2 package for the voice control functionality:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python whisper_voice_control\ncd whisper_voice_control\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Update the ",(0,r.jsx)(n.code,{children:"setup.py"})," file to include the executable:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nfrom glob import glob\nfrom setuptools import setup\nfrom setuptools import find_packages\n\npackage_name = 'whisper_voice_control'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n        (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='Your Name',\n    maintainer_email='your.email@example.com',\n    description='Voice control using OpenAI Whisper',\n    license='Apache License 2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'whisper_node = whisper_voice_control.whisper_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"bridge-architecture",children:"Bridge Architecture"}),"\n",(0,r.jsx)(n.p,{children:"The voice command bridge works as follows:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Capture"}),": PyAudio captures real-time audio from the microphone"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Processing"}),": Audio is normalized and buffered for processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transcription"}),": Whisper processes the audio buffer and generates text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Integration"}),": Transcribed text is published to the ",(0,r.jsx)(n.code,{children:"/voice_commands"})," topic"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Processing"}),": Other ROS 2 nodes can subscribe to ",(0,r.jsx)(n.code,{children:"/voice_commands"})," to execute voice-activated actions"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"testing-the-voice-command-bridge",children:"Testing the Voice Command Bridge"}),"\n",(0,r.jsx)(n.p,{children:"You can test the voice command bridge by running the node and listening to the topic:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start the whisper node\nros2 run whisper_voice_control whisper_node\n\n# Terminal 2: Listen to voice commands\nros2 topic echo /voice_commands std_msgs/msg/String\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Selection"}),": Smaller models (tiny, base) are faster but less accurate; larger models are more accurate but slower"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Buffering"}),": Balance between responsiveness and accuracy by adjusting buffer size"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Reduction"}),": Consider adding preprocessing steps for noise reduction in noisy environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Constraints"}),": For real-time applications, consider using faster models or optimizing the audio processing pipeline"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations-1",children:"Performance Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Selection"}),": Smaller models (tiny, base) are faster but less accurate; larger models are more accurate but slower"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Buffering"}),": Balance between responsiveness and accuracy by adjusting buffer size"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Reduction"}),": Consider adding preprocessing steps for noise reduction in noisy environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Constraints"}),": For real-time applications, consider using faster models or optimizing the audio processing pipeline"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,r.jsx)(n.p,{children:"When implementing voice command systems, it's important to include validation and safety checks:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def validate_voice_command(self, command_text):\n    """Validate voice commands before processing"""\n    # Filter out potentially harmful commands\n    forbidden_keywords = ["shutdown", "emergency", "kill", "stop all"]\n\n    for keyword in forbidden_keywords:\n        if keyword.lower() in command_text.lower():\n            self.get_logger().warning(f"Potentially harmful command blocked: {command_text}")\n            return False\n\n    # Validate command length to prevent buffer overflow attacks\n    if len(command_text) > 200:  # arbitrary limit\n        self.get_logger().warning(f"Command too long: {len(command_text)} chars")\n        return False\n\n    # Additional validation can be added here\n    return True\n'})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"In the next chapters, we'll explore how to process these voice commands using LLMs to generate ROS 2 actions and integrate with vision systems for a complete Vision-Language-Action pipeline."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>t});var s=i(6540);const r={},o=s.createContext(r);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);